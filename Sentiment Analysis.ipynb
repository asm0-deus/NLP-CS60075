{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "NLP_CS60075_A21_Assn2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "interpreter": {
      "hash": "23300995598eec4bcf6bd89cf02d1c3675e8b2616661418dbbf5580aa901878d"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment-2 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 15th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Wednesday, 22nd Sept, 2021 \n",
        "#### Submit this .ipynb file, named as `<Your_Roll_Number>_Assn2_NLP_A21.ipynb`"
      ],
      "metadata": {
        "id": "zyJ25uz0kSaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.  This dataset consists of 50k movie reviews (25k positive, 25k negative). You can download the dataset from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\r\n",
        "\r\n"
      ],
      "metadata": {
        "id": "Ao1nhg9RknmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please submit with outputs. "
      ],
      "metadata": {
        "id": "ONM5Q4SCe9Mr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import re\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import keras\r\n",
        "from sklearn.metrics import classification_report"
      ],
      "outputs": [],
      "metadata": {
        "id": "ElRkQElWUMjG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "filetrain=pd.read_csv(\"IMDB Dataset.csv\")\r\n",
        "col1=filetrain['review']\r\n",
        "col2=filetrain['sentiment']\r\n",
        "for i in range(2):\r\n",
        "    print(col1[i],\" \",col2[i])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.   positive\n",
            "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.   positive\n"
          ]
        }
      ],
      "metadata": {
        "id": "fhHRim2AUm4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "PrePrecessing that needs to be done on lower cased corpus\n",
        "\n",
        "1. Remove html tags\n",
        "2. Remove URLS\n",
        "3. Remove non alphanumeric character\n",
        "4. Remove Stopwords\n",
        "5. Perform stemming and lemmatization\n",
        "\n",
        "You can use regex from re. "
      ],
      "metadata": {
        "id": "lK_Hn2f6VMP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "text1=[]\r\n",
        "for i in range(len(col1)):\r\n",
        "    soup = BeautifulSoup(col1[i])\r\n",
        "    text = soup.get_text()\r\n",
        "    text1.append(text)         #Removed tags and urls\r\n",
        "    \r\n",
        "# for i in range(2):\r\n",
        "#     print(text1[i])"
      ],
      "outputs": [],
      "metadata": {
        "id": "5B5lHZPsVOXv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt') # For tokenizers\r\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "words=[]\r\n",
        "sum=0\r\n",
        "stop_words=set(stopwords.words('english'))\r\n",
        "for i in range(len(text1)):\r\n",
        "    words.append(nltk.word_tokenize(text1[i]))\r\n",
        "    words[i]=[word.lower() for word in words[i] if word.isalnum()]   #removed non alphanumeric\r\n",
        "    words[i]=[word.lower() for word in words[i] if word not in stop_words]  #removed stopwords\r\n",
        "    sum=sum+len(words[i])\r\n",
        "    # print(len(words[i]))\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "sum2=0\r\n",
        "words2=words\r\n",
        "words3=[]\r\n",
        "Max=0\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "for i in range(len(words2)):\r\n",
        "    tempstr=\"\"\r\n",
        "    for wd in range(len(words2[i])):\r\n",
        "        words2[i][wd]=lemmatizer.lemmatize(words2[i][wd]) \r\n",
        "        tempstr=tempstr+' '+words2[i][wd]\r\n",
        "    words3.append(tempstr)\r\n",
        "    # words2[i]=set(words2[i])\r\n",
        "    sum2=sum2+(len(words2[i]))       #lemmatization completed\r\n",
        "    if(len(words2[i])>Max):\r\n",
        "        Max=len(words2[i])\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\r\n",
        "# print(len(col1))\r\n",
        "count=0\r\n",
        "cnt=0\r\n",
        "for i in col2:\r\n",
        "    if(i=='positive'):\r\n",
        "        count=count+1\r\n",
        "    if(i=='negative'):\r\n",
        "        cnt=cnt+1\r\n",
        "print(\"Positive reviews = \",count,\"\\nNegative reviews = \",cnt)\r\n",
        "print(\"average no. of words per review = \",sum/len(col2))\r\n",
        "print(\"maximum number of words = \",Max)\r\n",
        "# print(\"average no. of words after lemmatization = \",sum2/len(col2))\r\n",
        "# print(col2.count(\"positive\"))\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive reviews =  25000 \n",
            "Negative reviews =  25000\n",
            "average no. of words per review =  113.89472\n",
            "maximum number of words =  1404\n"
          ]
        }
      ],
      "metadata": {
        "id": "DyaSkfcvYGXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes classifier"
      ],
      "metadata": {
        "id": "_FkJ-e2pUwun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# get reviews column from df\r\n",
        "reviews = words3\r\n",
        "\r\n",
        "# get labels column from df\r\n",
        "labels = col2"
      ],
      "outputs": [],
      "metadata": {
        "id": "eVq-mN28U_J4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Use label encoder to encode labels. Convert to 0/1\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoded_labels = encoder.fit_transform(labels)\r\n",
        "print((encoded_labels))\r\n",
        "print(encoder.classes_)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 0 0 0]\n",
            "['negative' 'positive']\n"
          ]
        }
      ],
      "metadata": {
        "id": "Ljo5NquhXTXr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# Split the data into train and test (80% - 20%). \r\n",
        "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\r\n",
        "\r\n",
        "train_sentences, test_sentences, train_labels, test_labels=train_test_split(reviews,encoded_labels,test_size=0.2)\r\n",
        "# print((train_labels[:5]),(test_labels[:5]))\r\n",
        "# print(type(train_sentences))"
      ],
      "outputs": [],
      "metadata": {
        "id": "wzG-C_EVWWET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here there are two approaches possible for building vocabulary for the naive Bayes.\n",
        "1. Take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
        "2. Take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n",
        " \n",
        "You are supposed to go by the 2nd approach.\n",
        " \n",
        "Also building vocab by taking all words in the train set is memory intensive, hence you are required to build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
        "\n",
        "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
        "\n",
        "\n",
        "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
        "\n",
        "$N_{w_j}$ : Total count of features in class $w_j$\n",
        "\n",
        "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
        "\n",
        "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bz1YdsSkiWCX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "# Use Count vectorizer to get frequency of the words\r\n",
        "\r\n",
        "# max_features parameter : If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\r\n",
        "\r\n",
        "vec = CountVectorizer(max_features = 3000)\r\n",
        "X = vec.fit_transform(train_sentences)\r\n",
        "X=X.toarray()\r\n",
        "# Y = vec.fit_transform(test_sentences)\r\n",
        "print(((X[0])))\r\n",
        "# print(type(vec.get_feature_names()))\r\n",
        "vocab=vec.get_feature_names()\r\n",
        "# print(vocab)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "metadata": {
        "id": "1cllNfGmUr77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# Use laplace smoothing for words in test set not present in vocab of train set\r\n",
        "def lap():\r\n",
        "    ans=1/6000\r\n",
        "    return ans\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "qzRvPjWaWUnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# Build the model. Don't use the model from sklearn\r\n",
        "pos_count=[0]*3000\r\n",
        "neg_count=[0]*3000\r\n",
        "for i in range(len(train_labels)):\r\n",
        "    if(train_labels[i]==0):\r\n",
        "        neg_count=neg_count+X[i]\r\n",
        "    else:\r\n",
        "        pos_count=pos_count+X[i]\r\n",
        "\r\n",
        "print(pos_count)\r\n",
        "print(neg_count)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1619  322  163 ...  138  372  127]\n",
            "[1734  334  132 ...  349 1105   99]\n"
          ]
        }
      ],
      "metadata": {
        "id": "iE7pxWIYW1z0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# Test the model on test set and report Accuracy\r\n",
        "\r\n",
        "vec2=CountVectorizer(max_features=3000)\r\n",
        "Y=vec2.fit_transform(test_sentences)\r\n",
        "Y=Y.toarray()\r\n",
        "testvocab=vec2.get_feature_names()\r\n",
        "pos_prob=[0]*10000\r\n",
        "neg_prob=[0]*10000\r\n",
        "pred=[0]*10000\r\n",
        "for i in range(len(test_sentences)):\r\n",
        "    for j in range(len(testvocab)):\r\n",
        "        if(Y[i][j]>0):\r\n",
        "            if(testvocab[j] in vocab):\r\n",
        "                pos_prob[i]=pos_prob[i]+(pos_count[vocab.index(testvocab[j])]+1)/6000\r\n",
        "                neg_prob[i]=neg_prob[i]+(neg_count[vocab.index(testvocab[j])]+1)/6000\r\n",
        "            else:\r\n",
        "                pos_prob[i]=pos_prob[i]+1/6000\r\n",
        "                neg_prob[i]=neg_prob[i]+1/6000\r\n",
        "    if(pos_prob[i]>neg_prob[i]):\r\n",
        "        pred[i]=1\r\n",
        "    # if(pred[i]==test_labels[i]):\r\n",
        "    #     print(\"Match\")\r\n",
        "    # else:\r\n",
        "    #     print(\"Wrong\")\r\n",
        "    \r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "AtQSl1zvW4DD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "score=0\r\n",
        "for i in range(len(pred)):\r\n",
        "    if(pred[i]==test_labels[i]):\r\n",
        "        score=score+1\r\n",
        "print(\"correct predictions = \",score)\r\n",
        "print(\"wrong predictions = \",len(pred)-score)\r\n",
        "print(\"Accuracy in percent = \", score/100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct predictions =  7246\n",
            "wrong predictions =  2754\n",
            "Accuracy in percent =  72.46\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *LSTM* based Classifier\n",
        "\n",
        "Use the above train and test splits."
      ],
      "metadata": {
        "id": "WlNql0acU7sa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# Hyperparameters of the model\r\n",
        "vocab_size = 3000 # choose based on statistics\r\n",
        "oov_tok = '<OOK>'\r\n",
        "embedding_dim = 100\r\n",
        "max_length = 200# choose based on statistics, for example 150 to 200\r\n",
        "padding_type='post'\r\n",
        "trunc_type='post'"
      ],
      "outputs": [],
      "metadata": {
        "id": "SkqnvbUOXoN0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# tokenize sentences\r\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\r\n",
        "tokenizer.fit_on_texts(train_sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "\r\n",
        "# convert train dataset to sequence and pad sequences\r\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\r\n",
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\r\n",
        "\r\n",
        "# convert Test dataset to sequence and pad sequences\r\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\r\n",
        "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UeycEg9nZAOF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "# model initialization\r\n",
        "model = keras.Sequential([\r\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\r\n",
        "    keras.layers.Dense(24, activation='relu'),\r\n",
        "    keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        "# compile model\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "# model summary\r\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 100)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                3096      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 387,601\n",
            "Trainable params: 387,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "Mtw3w895ZP39"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "num_epochs = 5\r\n",
        "history = model.fit(train_padded, train_labels, \r\n",
        "                    epochs=num_epochs, verbose=1, \r\n",
        "                    validation_split=0.1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1125/1125 [==============================] - 97s 84ms/step - loss: 0.3943 - accuracy: 0.8243 - val_loss: 0.3142 - val_accuracy: 0.8658\n",
            "Epoch 2/5\n",
            "1125/1125 [==============================] - 106s 94ms/step - loss: 0.2913 - accuracy: 0.8811 - val_loss: 0.3089 - val_accuracy: 0.8705\n",
            "Epoch 3/5\n",
            "1125/1125 [==============================] - 113s 100ms/step - loss: 0.2535 - accuracy: 0.8990 - val_loss: 0.3212 - val_accuracy: 0.8723\n",
            "Epoch 4/5\n",
            "1125/1125 [==============================] - 109s 97ms/step - loss: 0.2242 - accuracy: 0.9117 - val_loss: 0.3250 - val_accuracy: 0.8658\n",
            "Epoch 5/5\n",
            "1125/1125 [==============================] - 108s 96ms/step - loss: 0.1942 - accuracy: 0.9239 - val_loss: 0.3547 - val_accuracy: 0.8673\n"
          ]
        }
      ],
      "metadata": {
        "id": "skmaDJMnZTzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "# Calculate accuracy on Test data\r\n",
        "\r\n",
        "prediction = model.predict(test_padded)\r\n",
        "\r\n",
        "\r\n",
        "# Get probabilities\r\n",
        "prob=[]\r\n",
        "for i in range(len(prediction)):\r\n",
        "    val=prediction[i][0]\r\n",
        "    prob.append(val)\r\n",
        "\r\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\r\n",
        "lstmlabels=[]\r\n",
        "for i in range(len(prob)):\r\n",
        "    if(prob[i]>=0.5):\r\n",
        "        ans=1\r\n",
        "    else:\r\n",
        "        ans=0\r\n",
        "    lstmlabels.append(ans)\r\n",
        "\r\n",
        "# Accuracy : one can use classification_report from sklearn\r\n",
        "target_names=['negative','positive']\r\n",
        "print(classification_report(test_labels,lstmlabels,target_names=target_names))\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.88      0.87      0.87      5045\n",
            "    positive       0.87      0.88      0.87      4955\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "TjEhWEr5Zq7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get predictions for random examples"
      ],
      "metadata": {
        "id": "TIICV-ySOYL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "# reviews on which we need to predict\r\n",
        "sentence = [\"The movie was very touching and heart whelming\", \r\n",
        "            \"I have never seen a terrible movie like this\", \r\n",
        "            \"the movie plot is terrible but it had good acting\"]\r\n",
        "\r\n",
        "# convert to a sequence\r\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\r\n",
        "\r\n",
        "# pad the sequence\r\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\r\n",
        "\r\n",
        "# Get probabilities\r\n",
        "print(\"Probabilities\\n\")\r\n",
        "randomprob=model.predict(padded)\r\n",
        "print(randomprob)\r\n",
        "randomlabels=[]\r\n",
        "\r\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\r\n",
        "for i in range(3):\r\n",
        "    if(randomprob[i][0]>=0.5):\r\n",
        "        randomlabels.append(1)\r\n",
        "    else:\r\n",
        "        randomlabels.append(0)\r\n",
        "for i in range(len(randomlabels)):\r\n",
        "    if(randomlabels[i]==1):\r\n",
        "        print('positive')\r\n",
        "    else:\r\n",
        "        print('negative')\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities\n",
            "\n",
            "[[0.94524586]\n",
            " [0.13283944]\n",
            " [0.13581777]]\n",
            "positive\n",
            "negative\n",
            "negative\n"
          ]
        }
      ],
      "metadata": {
        "id": "m2RmfNL3OYL0"
      }
    }
  ]
}